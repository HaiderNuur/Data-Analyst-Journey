# Week one
- Interpreted languages, such as R, python, and JS are a type of programming language that is interpreted and executed directly by an interpreter without first compiling it into the target machine instructions. Typically we access these languages through a command-line interpreter.
- R was released in 1994 by Bell lab
- The meain reason to use R is to perform very comprehensive statistical computation tasks. 
- we can also use R to perform data sampling to obtain a representative data subset
- can also use R to analyze if variables are correlated, perform hypothesis testing and infer the properties of unknown distributions.
- also used in Data viz, perform interactive and visual analytics, predictive analysis, machine learning and AI tasks.
- statistical computation
- analyze the zip related data on an interactive map
# Week four
- You can read a CSV file or an Excel file into a dataset using the read_csv() and read_excel() functions and then access its rows, columns, and individual data points.
- The readLines() function reads a text file into a character vector. 
- The length() and nchar() functions return information about a character vector.  
- The scan() function reads a text file into a character vector with each individual word as an element. 
- The write()function exports a dataset as a text file. 
- The write.csv() and write.table() functions export a dataset as a .csv file. 
- The write.xlsx() function exports a dataset as an Excel file. 
- The save() function saves R objects in .RData files. 
- HTTP is a communication protocol for fetching web resources for clients from servers on the Internet and that each instance is comprised of a request and response. 
- The REST API is a web service that uses the REST architecture to handle a request on a frontend web service.  
- The httr package in R has functions that perform common HTTP and REST operations. 
- The rvest package in R has functions that you can use to perform common web scraping tasks, such as reading HTML from a character variable, reading HTML from a URL, downloading a web page and reading it offline, extracting node data from a web page, and converting a table from a web page to a data frame. 
