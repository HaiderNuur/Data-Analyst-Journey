07.02.2021

# Week 1
Data Analyst Role: 
Responsibilities of a Data Analyst:
-	Acquiring data from primary & secondary sources
-	Creating queries to extract required data from databases and other data collection systems
-	filtering, cleaning, standardizing, and reorganizing data in preparation for analysis
-	Using statistical tools to interpret datasets
-	Using statistical techniques to identify patterns and correlations in data
-	analyze datasets and complex patterns to interpret trends
-	preparing reports and charts that effectively communicate trends & patterns
-	create documentation to define and demonstrate the data analysis process

Technical skills
-	Expertise in using spreadsheets (Microsoft Excel or Google Sheets)
-	Proficiency in statistical analysis and visualization tools and software (IBM cognos, IBM SPSS, Oracle Visual Analyzer, Microsoft Power BI, SAS, Tableau)
-	Proficiency in programming languages (R, python, C++, JAVA, MATLAB)
-	Good Knowledge of SQL and ability to work with data in relational and NoSQL databases
-	The ability to access and extract data from data repositories (Data Marts, Data Warehouses, Data Lakes & Data Pipelines)
-	Familiarity with Big Data processing tools (Hadoop, Hive & Spark)
Functional skills
-	Proficiency in Statistics (Analyze data, validate the analysis, identify fallacies and logical errors)
-	Analytical skills (research and interpret data, theorize, make forecasts)
-	Problem Solving skills (end goal of data analysis is to solve a given problem)
-	Probing Skills (identify and define the problem statement and desired outcome, understand the problem from the stakeholder and users)
-	data visualization skills (Create clear and compelling visualizations to present the analysis)
-	project management skills(manage the process, people, dependencies, and timelines of the initiatives)

Soft skills
-	Your ability to: work collaboratively with business and cross-functional teams, communicate effectively to report and present your findings, tell a compelling and convincing story, gather support and buy-in for your work
-	Curiosity (allowing new questions to surface and challenging your own assumptions and hypothesis)
-	Intuition (having a sense of the future based on pattern recognition and past experiences)
What are the qualities and skills required to be a data analyst?
- Natural curiosity (looking for answers even when there isn't a question, looking in areas that may not have been thought of before)
-	attention to detail (looking for patterns, paying attention to close details)
-	enjoy working with computers ( develop new skills)
-	technical skills (python, sql, r, tableau, power BI)
-	Decisions regarding the right data and right tools to use
-	know how to present data to key stakeholders ( business acumen, presentation skills)
-	should be detail-oriented
-	love numbers and information ( just not looking information, dive deeper)
-	an eye and mindset to catch things that don't look right
-	Soft skills ( curiosity, thoughtfulness, ability to listen carefully, ability to understand, both user and co-worker perspective, willingness to learn)
-	technical skills (***SQL, python & R, data visualization tool )
-	Know what problem is to be solved with data
-	pull data in the required structure from the data lake, using SQL
-	clean, wrangle, manipulate, and mine data to glean insights
-	present insights concisely and clearly using good visualizations and dashboards
-	tell a good story with data

A day in the life of a data analyst.
-	acquiring data from varied sources
-	creating queries for fetching data from data repositories
-	looking for insights in data
-	creating reports and dashboards
-	interacting with stakeholders for gathering information and
presenting findings
-	cleaning and preparing data for analysis

A particular task - case study: Looking for insights.
-	A client, a power company in somewhere, has been noticing a spike in complaints regarding overbilling. the frequency seems there's something to look deeper.
-	Data analyst role: Look at the complaints and billing data to find out any possible insights
Start with working on hypotheses:
-	Obvious places to look at : complain data, subscriber information data, billing data.
Before looking at data, should prepare some questions as initial hypotheses.
-	Is there a consumption range for which overbilling is occurring more than others?
-	Area wise concentration of complaints - are the complaints concentrated in specific localities within the city?
-	Frequency and occurrence of complaints based on individual subs: are the same subscribers reporting overbilling repetitive?
-	if yes, what is the frequency of occurrence of repeat cases? If a subscriber gets overbilling a month, does he receive overbilling more?

As analyst is clear about initial hypotheses and set of questions 
-	to start with, next job is to identify the datasets that he is going to isolate and analyze to validate or refute the hypotheses. such as - 
-	pull out the annual, quarterly, and monthly billing amounts of the complaints and look for a range in which the complaints are falling more than others.
-	pulling location data to see is there a connection between overbilling and zip codes?
-	see that what concentration is there of overbilling in certain areas.
-	pull out the date of connection data -  more than 95% of the complaints had been our subscribers for more than seven years.
-	according to area wise concentration, and significant concentration of complaints based on date of connection
-	pull out make and serial number of the meters, see  that the make and the serial number of the meters belong to a single supplier.
-	Feel confident to present these findings to the stakeholders, data sources and process of finding.
-	
# Week 2
-	Data can come in variety of file formats: relational, non-relational database, APIs, Web services, data streams, social platforms, sensor devices.
-	Data repositories: Databases, Data warehouses, Data Marts, Data lakes, Big data sources.
-	Type, file format & source of data influence the type of data repositories that you would collect, clean, mine for analysis
-	Languages: Query Language (SQL for querying and manipulating data), Programming Language (Python for developing data applications), Shell and scripting language ( for repetitive operational tasks)
-	Structured data examples – SQL databases, Online Transaction Processing (OLTP), Spreadsheets, Online forms, Sensors GPS and RFID, Network and Web server logs.
-	Semi-structured data – Emails, XML and other markup languages, Binary executables, TCP/IP packets, Zipped files, Integration of data.
-	XML and JSON allows users to define tags and attributes to store data in a hierarchical form and are widely used to store and exchange semi-structured data.
-	Unstructured data – web pages, social media feeds, images in varied file formats, video and audio files, documents and pdf files, powerpoint presentations, media logs, surveys.
-	Unstructured data can be stored in files/documents for manual analysis or in NoSQL databases that has analysis tools to execute such data.
-	Delimited Text Files: Files used to store data as text. Each value is separated by a delimiter like comma (csv or comma separated file), tab (tsb), colon, vertical bar, space.
-	JavaScript Object Notation, or JSON, is a text-based open standard designed for transmitting structured data over the web. Language-independent data format, can be read in any programming language, easy to use, compatible with a wide range of browsers, considered as one of the best tools for sharing data.
Sources of Data
-	Relational databases – Typically organizations have internal applications to support them in managing business activities, customer transactions, human resource activities and their workflows. These systems use relational These systems use relational databases such as SQL Server, Oracle, MySQL, and IBM DB2, to store data in a structured way. Data stored in databases and data warehouses can be used as a source for analysis. For example, data from a retail transactions system can be used to analyze sales in different regions, and data from a customer relationship management system can be used for making sales projections.

-	Flat File and XML Datasets: External to the organization, there are other publicly and privately available datasets. For example, government organizations releasing demographic and economic datasets on an ongoing basis. Then there are companies that sell specific data, for example, Point-of-Sale data or Financial data, or Weather data, which businesses can use to define strategy, predict demand, and make decisions related to distribution or marketing promotions, among other things. Such data sets are typically made available as flat files, spreadsheet files, or XML documents. Flat files, store data in plain text format, with one record or row per line, and each value separated by delimiters such as commas, semi-colons or tabs. Data in a flat file maps to a single table, unlike relational databases that contain multiple tables. One of the most common flat file format is CSV in which values are separated by commas. Spreadsheet files are a special type of flat files, that also organize data in a tabular format – rows and columns. But a spreadsheet can contain multiple worksheets, and each worksheet can map to a different table. Although data in spreadsheets is in plain text, the files can be stored in custom formats and include additional information such as formatting, formulas, etc. Microsoft Excel, which stores data in .XLS or .XLSX format is probably the most common spreadsheet. Others include Google sheets, Apple Numbers, and LibreOffice. XML files, contain data values that are identified or marked up using tags.  While data in flat files is “flat” or maps to a single table, XML files can support  more complex data structures, such as hierarchical. Some common uses of XML include data from online surveys, bank statements, and other  unstructured data sets.

-	APIs and Web services: Many data providers and websites provide APIs, or Application Program Interfaces, and Web Services, which multiple users or applications can interact with and obtain data for processing or analysis. APIs and Web Services typically listen for incoming requests, which can be in the form of web requests from users or network requests from applications and return data in plain text, XML, HTML, JSON, or media files. Let’s look at some popular examples of APIs being used as a data source for data analytics: The use of Twitter and Facebook APIs to source data from tweets and posts for performing tasks such as opinion mining or sentiment analysis, which is to summarize the amount of appreciation and criticism on a given subject, such as policies of a government, a product, a service, or customer satisfaction in general. Stock Market APIs used for pulling data such as share and commodity prices, earnings per share, and historical prices, for trading and analysis. Data Lookup and Validation APIs, which can be very useful for Data Analysts for cleaning and preparing data, as well as for co-relating data—for example, to check which city or state a postal or zip code belongs to. APIs are also used for pulling data from database sources, within and external to the organization.

-	Web scrapping: Web scraping is used to extract relevant data from unstructured sources. Also known as screen scraping, web harvesting, and web data extraction, web scraping makes it possible to download specific data from web pages based on defined parameters. Web scrapers can, among other things, extract text, contact information, images, videos, product items, and much more from a website. Some popular uses of web scraping include: collecting product details from retailers, manufacturers, and eCommerce websites to provide price comparisons, generating sales leads through public data sources, extracting data from posts and authors on various forums and communities, and collecting training and testing datasets for machine learning models. Some of the popular web scraping tools include BeautifulSoup, Scrapy, Pandas, and Selenium. Data streams are another widely used source for aggregating constant streams of data flowing from sources such as instruments, IoT devices and applications, GPS data from cars, computer programs, websites, and social media posts. This data is generally timestamped and also geo-tagged for geographical identification. Some of the data streams and ways in which they can be leveraged include: stock and market tickers for financial trading, retail transaction streams for predicting demand and supply chain management, surveillance and video feeds for threat detection, social media feeds for sentiment analysis, sensor data feeds for monitoring industrial or farming machinery, web click feeds for monitoring web performance and improving design, and real-time flight events for rebooking and rescheduling. Some popular applications used to process data streams include Apache Kafka, Apache Spark Streaming, and Apache Storm. RSS (or Really Simple Syndication) feeds, are another popular data source. These are typically used for capturing updated data from online forums and news sites where data is refreshed on an ongoing basis. Using a feed reader, which is an interface that converts RSS text files into a stream of updated data, updates are streamed to user devices.
12/2/21
-	Popular RDBMS – IBM DB2, Microsoft SQL server, MySQL, Oracle DB, Postgre SQL.
-	Cloud based RDB – Amazon RDS, Google SQL, IBM DB2 on cloud, Oracle Cloud, Azure SQL.
-	Key-value based NoSQL databases – Radis, Memcached, DynamoDB
-	Document based NoSQL – MongoDB, DocumentDB, CouchDB, Cloudant
-	Column based NoSQL – Cassandra, Hbase
-	Graph based NoSQL – Neo4J, CosmosDB
-	Data lake - Data lakes: A data lake is a pool of raw data where each data element is given a unique identifier and is tagged with metatags for further use, data from a data lake is selected and organized based on the use case you need it for, a data lake retains all source data without exclusions, most important role is predictive and advanced analytics.
-	A data pipeline - encompasses the entire journey of moving data from one system to another, including the ETL process, can be used for both batch and streaming data, supports both long-running batch queries and smaller. destination is data lakes. 
In this lesson, you have learned the following information: 
A Data Repository is a general term that refers to data that has been collected, organized, and isolated so that it can be used for reporting, analytics, and also for archival purposes.  
The different types of Data Repositories include: 
•	Databases, which can be relational or non-relational, each following a set of organizational principles, the types of data they can store, and the tools that can be used to query, organize, and retrieve data.
•	Data Warehouses, that consolidate incoming data into one comprehensive storehouse.  
•	Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case. 
•	Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their native format. 
•	Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.
ETL, or Extract Transform and Load, Process is an automated process that converts raw data into analysis-ready data by:
•	Extracting data from source locations.
•	Transforming raw data by cleaning, enriching, standardizing, and validating it.
•	Loading the processed data into a destination system or data repository.
Data Pipeline, sometimes used interchangeably with ETL, encompasses the entire journey of moving data from the source to a destination data lake or application, using the ETL process.  
Big Data refers to the vast amounts of data that is being produced each moment of every day, by people, tools, and machines. The sheer velocity, volume, and variety of data challenge the tools and systems used for conventional data. These challenges led to the emergence of processing tools and platforms designed specifically for Big Data, such as Apache Hadoop, Apache Hive, and Apache Spark.
13/02/21
-	Process for identifying data: Determine the information you want to collect


# WHAT a GAP! From February and now end of October


28.10.2021
WEEK 3: Gathering and Wrangling Data
Process of Identifying data for analysis: 
-	Determine the information you want to collect: The specific information you need and The possible sources for this data
-	Define a plan for collecting data (The purpose is to establish the clarity you need for execution)– 1. Establish a timeframe for collecting data, 2. How much data is sufficient for a credible data analysis 3. Define dependencies, risks and mitigation plan
-	Determine your data collection methods: The methods depend on sources of data, type of data, timeframe over which you need the data and volume of data.
-	Key considerations for the life cycle of data analysis: The data you identify, the source of data, and the practices you employ for gathering the data have implications for quality, security and privacy.
-	Data quality: should be free of errors, accurate, complete, relevant and accessible
-	Data governance: Issues pertaining to data governance include security, regulation and compliances. Data governance policies and procedures relate to the usability, integrity, and availability of data.
-	Data privacy: You need to define checks, validations, and auditable trail for issues confidentiality, license  for use and compliance to mandated regulations.
Data Sources
-	Data sources can be internal or external to the organization. It can be primary, secondary or third-party sources of data. 
-	Primary data: information obtained directly from the source (Internal db, Data from the organization’s CRM< HR, or workflow applications, or data you gather directly through surveys, interviews, discussions, or observations or focus groups).
-	Secondary Data: information retrieved from existing sources. (External Databases, research articles, publications, training material, internet searches or financial records available as publica data, or data collected through externally conducted surveys, interviews, discussions, observations or focus groups
-	Third-party data: purchased from aggregators who collect data from various sources and combine it into comprehensive datasets for selling
-	Sources For gathering Data: 
-	
-	


